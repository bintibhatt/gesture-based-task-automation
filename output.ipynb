{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, lite\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import datetime\n",
    "from operator import index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands  # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils  # Drawing utilities\n",
    "\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    # COLOR CONVERSION BGR 2 RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw left hand connections\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(\n",
    "                                      color=(111, 22, 76), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(\n",
    "                                      color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "                                  )\n",
    "\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        # Draw left hand connections\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.landmark[point].x, res.landmark[point].y, res.landmark[point].z]\n",
    "                  for point in mp_hands.HandLandmark for res in results.multi_hand_landmarks], dtype=float).flatten() if results.multi_hand_landmarks else np.zeros(21*3*2)\n",
    "    if lh.shape[0] == 63:\n",
    "        rh = np.array([[res.landmark[point].x, res.landmark[point].y, res.landmark[point].z]\n",
    "                      for point in mp_hands.HandLandmark for res in results.multi_hand_landmarks], dtype=float).flatten() if results.multi_hand_landmarks else np.zeros(21*3)\n",
    "        return np.concatenate([lh, rh])\n",
    "    return np.concatenate([lh])\n",
    "\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),   # Red\n",
    "    (0, 255, 0),   # Green\n",
    "    (0, 0, 255),   # Blue\n",
    "    (255, 255, 0),  # Yellow\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Cyan\n",
    "    (128, 128, 128)  # Gray\n",
    "]\n",
    "\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = cv2.resize(input_frame, (1000, 800))\n",
    "    cv2.rectangle(output_frame, (0, 0), (800, 60), (255, 255, 255), -1)\n",
    "    cv2.putText(output_frame, 'Gesture Detection', (10, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (50, 100 + num * 50),\n",
    "                      (50 + int(prob * 700), 130 + num * 50), colors[num], -1)\n",
    "        cv2.putText(output_frame, f'{actions[num]}: {prob:.2f}', (\n",
    "            60, 125 + num * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions that we try to detect\n",
    "actions = np.array(['thumbsup', 'thumbsdown', 'peace', 'palm', 'fist'])\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./action.h5\",compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Photo deleted: ./images-clicked/photo_20230430_183700.png\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.4) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:799: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24040/1306109400.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m                             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                             \u001b[0mbright_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m50.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                             \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbright_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Brightness increased for image:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.4) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:799: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n"
     ]
    }
   ],
   "source": [
    "# final code\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.9\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# counter = 1\n",
    "\n",
    "try:\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "        start_time = time.time()\n",
    "        while cap.isOpened():\n",
    "\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, hands)\n",
    "\n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            # Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]\n",
    "\n",
    "            if len(sequence) == 30:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                predictions.append(np.argmax(res))\n",
    "\n",
    "                # Viz logic\n",
    "                if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "                    if res[np.argmax(res)] > threshold:\n",
    "\n",
    "                        word = \"\"\n",
    "                        if len(sentence) > 0:\n",
    "                            if actions[np.argmax(res)] != sentence[-1]:\n",
    "                                sentence.append(actions[np.argmax(res)])\n",
    "                                word = actions[np.argmax(res)]\n",
    "                        else:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                            word = actions[np.argmax(res)]\n",
    "\n",
    "                        if word == \"thumbsup\":\n",
    "                            delay = 5000\n",
    "                            filename = \"./images-clicked/photo_{}.png\".format(timestamp)\n",
    "                            cv2.imwrite(filename, frame)\n",
    "                            print(\"Photo saved as\", filename)\n",
    "                            # Break out of the loop after taking one photo\n",
    "                            delay = 10000\n",
    "\n",
    "                        if word == \"palm\":\n",
    "                            # Show the latest photo\n",
    "                            cv2.imshow(filename, cv2.imread(filename))\n",
    "\n",
    "                            # Set delay time\n",
    "                            delay = 10000\n",
    "\n",
    "                        if word == \"peace\":\n",
    "                            img = cv2.imread(filename)\n",
    "                            bright_img = cv2.add(img, np.array([50.0]))\n",
    "                            cv2.imwrite(filename, bright_img)\n",
    "                            print(\"Brightness increased for image:\", filename)\n",
    "                        \n",
    "                        if word == \"fist\":\n",
    "                            img = cv2.imread(filename)\n",
    "                            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                            cv2.imwrite(filename, gray_img)\n",
    "                            print(\"Image converted to grayscale:\", filename)\n",
    "\n",
    "                        if word == \"thumbsdown\":\n",
    "                            # Delete the photo if the gesture is thumbs up\n",
    "                            if os.path.exists(filename):\n",
    "                                os.remove(filename)\n",
    "                                print(\"Photo deleted:\", filename)\n",
    "                            else:\n",
    "                                print(\"File does not exist:\", filename)\n",
    "\n",
    "                if len(sentence) > 1:\n",
    "                    sentence = sentence[-1:]\n",
    "\n",
    "                # Viz probabilities\n",
    "                image = prob_viz(res, actions, image, colors)\n",
    "\n",
    "            # Show to screen\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
